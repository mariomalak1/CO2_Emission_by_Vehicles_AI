# -*- coding: utf-8 -*-
"""Assignment1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K-zG7hEFGq-o5P_SxKDjJZVnogz5H0D6
"""

import numpy
import pandas
import seaborn as sns
import copy, math
import matplotlib.pyplot as plt
from sklearn.metrics import r2_score
from sklearn.linear_model import SGDClassifier
from sklearn.metrics import accuracy_score

## Load the "co2_emissions_data.csv" dataset.
data = pandas.read_csv('co2_emissions_data.csv')
data.head()

# check missing values
data.isnull().sum()

# to show scale of features
data.describe()

sns.pairplot(data, diag_kind='hist')

data.info()

# create dataframe with numeric features only
numericFeatures = data.drop(["Make", "Model", "Vehicle Class", "Transmission", "Fuel Type", "Emission Class"],   axis=1).copy()
correlation_matrix = numericFeatures.corr()
#heat map
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)

# separate the features and targets
# features ->x
x=data.iloc[:,:-2].values
x

#targets
y_co2=data.iloc[:,-2]
y_co2

#targets
y_class=data.iloc[:,-1]
y_class

# categorical features and targets are encoded
from sklearn.preprocessing import LabelEncoder
# //take a copy of data to avoid modify the actual data frame
encoded_data = data.copy()

# label incoding because one hot generate many columns
label_encoder = LabelEncoder()
for column in encoded_data.columns:
# check if column is object type
    if encoded_data[column].dtype == 'object' :
        encoded_data[column] = label_encoder.fit_transform(encoded_data[column])


encoded_data
# y_class_encoded = label_encoder.fit_transform(y_class)
# y_class_encoded

from sklearn.model_selection import train_test_split
x_train,x_test,y_co2_train,y_co2_test,y_class_train,y_class_test=train_test_split(encoded_data,encoded_data.iloc[:,-2],encoded_data.iloc[:,-1],test_size=.3,random_state=0)
# x_train
# y_co2_train.shape
# y_class_train
y_class_train.shape
# x_test

# numeric features are scaled
from sklearn.preprocessing import MinMaxScaler
# normalization scale features between 0 :1
scaler = MinMaxScaler()
x_train=scaler.fit_transform(x_train)
x_test=scaler.transform(x_test)

# x_train
# x_test
x_train.max()

"""Implement linear regression using gradient descent from scratch"""

def compute_gradient(X, y, w, b):
    m = X.shape[0]
    predictions = numpy.dot(X, w) + b
    error = predictions - y
    dj_dw = numpy.dot(X.T, error) / m
    dj_db = numpy.sum(error) / m
    return dj_db, dj_dw

x_train_selected = x_train[:, [9, 3]]
y_train_co2 = y_co2_train.values
y_train_co2 = y_train_co2.reshape(-1)

b = 0
w_init = numpy.zeros(2)

def compute_cost(X, y, w, b):
    m = X.shape[0]
    predictions = numpy.dot(X, w) + b
    error = predictions - y
    cost = numpy.sum(error**2) / (2 * m)

    return cost

def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters):
    Cost_history = []  # List to store cost history
    w = copy.deepcopy(w_in)  # Copy of initial weights
    b = b_in                 # Initialize bias

    for i in range(num_iters):
        # Compute gradients
        dj_db, dj_dw = gradient_function(X, y, w, b)

        # Use temporary variables to update w and b
        temp_w = w - alpha * dj_dw
        temp_b = b - alpha * dj_db

        # Update w and b after calculating their new values
        w = temp_w
        b = temp_b

        # Save cost at each iteration
        if i < 100000:
            Cost_history.append(cost_function(X, y, w, b))

        # Print cost at intervals (10 times during training)
        if i % math.ceil(num_iters / 10) == 0:
            print(f"Iteration {i:4d}: Cost {Cost_history[-1]:8.2f}")

    return w, b, Cost_history

initial_w = w_init
initial_b =0
iterations=1000
alpha= 5.0e-1

# Run gradient descent
w_final, b_final, J_hist = gradient_descent(x_train_selected, y_train_co2, initial_w, initial_b,
                                             compute_cost, compute_gradient,
                                             alpha, iterations)

fig, ax1 = plt.subplots(figsize=(12, 4))

# Plotting the first graph with color
ax1.plot(J_hist, color='royalblue', lw=2, marker='o', markersize=4, markerfacecolor='red', markeredgewidth=1)

ax1.set_title("Cost vs. iteration", fontsize=16, fontweight='bold')
ax1.set_ylabel('Cost', fontsize=14)
ax1.set_xlabel('Iteration step', fontsize=14)
ax1.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)

plt.tight_layout()
plt.show()

x_test_selected = x_test[:, [9, 3]]
y_test_co2 = y_co2_test.values

y_test_predictions = numpy.dot(x_test_selected, w_final) + b_final

r2 = r2_score(y_test_co2, y_test_predictions)
print(f"R^2 score on the test set: {r2:.4f}")

"""fit logistic regression model"""

model = SGDClassifier()
model.fit(x_train_selected, y_class_train)
predictions = model.predict(x_test_selected)
accuracy_score(y_class_test, predictions)